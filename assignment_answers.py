# -*- coding: utf-8 -*-
"""assignment answers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wIMVXc8oZq8gqspzFq58d9jozFyG71gM
"""

##rite a script that takes a userâ€™s name as input and prints a greeting message in uppercase.
name = input("Please type your name: ").strip()
print("HELLO, " + name.upper() + "! WELCOME ")
print(f"Hello, {name.upper()}!")

###Write a Python script to parse a CSV file.

import pandas as pd
data = pd.read_csv('/content/ecommerce_dataset_updated.csv')
# Displaying first few rows
print(" Preview of CSV:")
print(data.head())
# DisplayIing column names
print("\n Columns in the CSV:")
print(list(data.columns))

# Q3: ETL Script - Load JSON Data into SQLite

import json
import pandas as pd
import sqlite3

# STEP 1: Sample JSON data
sample_json = [
    {"id": 1, "name": "Namratha", "email": "namratha@example.com", "age": 23},
    {"id": 2, "name": "Ravi", "email": "ravi@example.com", "age": 29},
    {"id": 3, "name": "Priya", "email": "priya@example.com", "age": 25}
]
# Saving it as a file
with open("data.json", "w") as f:
    json.dump(sample_json, f)
# STEP 2: Extracting - Read JSON
with open("data.json", "r") as f:
    data = json.load(f)

# STEP 3: Transform - Convert to DataFrame
df = pd.DataFrame(data)
print("Extracted & transformed data:\n")
print(df)

# STEP 4: Load - Write to SQLite database
conn = sqlite3.connect("etl_demo.db")  # Creates a local DB file
cursor = conn.cursor()

# Creating table
cursor.execute("""
CREATE TABLE IF NOT EXISTS users (
    id INTEGER PRIMARY KEY,
    name TEXT,
    email TEXT,
    age INTEGER
)
""")

# Inserting data
for record in data:
    cursor.execute("""
        INSERT OR IGNORE INTO users (id, name, email, age)
        VALUES (?, ?, ?, ?)
    """, (record["id"], record["name"], record["email"], record["age"]))

conn.commit()

#  STEP 5: Verifying data
print("\n Data loaded into SQLite table successfully!\n")
print(pd.read_sql("SELECT * FROM users", conn))

conn.close()

# Q5:  Implement a PySpark job to count word frequency from text files.

# Step 1: Installing PySpark
!pip install pyspark
# Step 2: Import and Initializing Spark
from pyspark import SparkContext
# Creating a local Spark context
sc = SparkContext("local", "WordCountApp")
#  Step 3: Load your text file
text_file = sc.textFile("/content/colb .txt")

#Step 4: Spliting lines into words, map each word to (word, 1), then reduce by key
word_counts = (
    text_file.flatMap(lambda line: line.split())
             .map(lambda word: (word.lower(), 1))
             .reduceByKey(lambda a, b: a + b)
)
# Step 5: Collecting and display results
for word, count in word_counts.collect():
    print(f"{word}: {count}")

# Step 6: Stopig Spark
sc.stop()